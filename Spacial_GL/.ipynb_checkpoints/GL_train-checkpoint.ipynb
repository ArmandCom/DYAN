{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "## Imports related to PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.misc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from pylab import imshow, show, get_cmap\n",
    "\n",
    "## Generic imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Dependencies classes and functions\n",
    "from utils import gridRing\n",
    "from utils import asMinutes\n",
    "from utils import timeSince\n",
    "from utils import getWeights\n",
    "#from utils import videoDataset\n",
    "from utils import save_checkpoint\n",
    "from utils import getListOfFolders\n",
    "\n",
    "## Import Model\n",
    "from DyanOF import OFModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to compute group lasso\n",
    "- Option 1: \n",
    "    - We compute c for a neighborhood, applying a factor for minimizing the 2-norm of coefficient inside a group.\n",
    "    - We input every local neighborhood as a frame\n",
    "    - We only keep the value of c for the main pixel\n",
    "    - Problem: computation increases a lot. For each frame a set of #pix c vectors is computed. Even if the new input is smaller we thing there are a lot of fixed computation costs in performing FISTA.\n",
    "- Option 2:\n",
    "    - Given the full frame we apply group lasso to force the c coefficients to be similar (2-norm) if they belong to the same neighborhood pixels.\n",
    "    - C's would be recomputed every time (?)\n",
    "    - Could be done for non-sliding windows (small)\n",
    "    - Problem: We would have to add the 2-norm of each group. Does Fista algorithm change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Dataloader for PyTorch.\n",
    "# class videoDataset(Dataset):\n",
    "#     \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "#     def __init__(self, folderList, rootDir, N_FRAME, N_FRAME_FOLDER): #N_FRAME = FRA+PRE\n",
    "\n",
    "#         self.listOfFolders = folderList\n",
    "#         self.rootDir = rootDir\n",
    "#         self.nfra = N_FRAME\n",
    "#         self.nfrafol = N_FRAME_FOLDER\n",
    "#         # self.numpixels = 240*320 # If Kitti dataset, self.numpixels = 128*160\n",
    "#         self.numpixels = 128*160 # MNIST moving symbols dataset\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.listOfFolders)\n",
    "\n",
    "#     def readData(self, folderName):\n",
    "#         path = os.path.join(self.rootDir,folderName)\n",
    "#         OF = torch.FloatTensor(2,self.nfrafol,self.numpixels)\n",
    "\n",
    "#         for framenum in range(self.nfrafol):\n",
    "#             flow = np.load(os.path.join(path,str(framenum)+'.npy'))\n",
    "#             flow = np.transpose(flow,(2,0,1))\n",
    "#             OF[:,framenum] = torch.from_numpy(flow.reshape(2,self.numpixels)).type(torch.FloatTensor)\n",
    "#         return OF\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         folderName = self.listOfFolders[idx]\n",
    "#         Frame = self.readData(folderName)\n",
    "#         sample = { 'frames': Frame }\n",
    "\n",
    "#         return sample\n",
    "    \n",
    "class videoDataset(Dataset):\n",
    "    \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "\n",
    "    def __init__(self, listOfFolders, rootDir):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            N_FRAME (int) : Number of frames to be loaded\n",
    "            rootDir (string): Directory with all the Frames/Videoes.\n",
    "        \"\"\"\n",
    "        self.listOfFolders = listOfFolders\n",
    "        self.rootDir = rootDir\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listOfFolders)\n",
    "\n",
    "    def readData(self, folderName):\n",
    "        path = os.path.join(self.rootDir,folderName)\n",
    "        numBatches = 1#min(int(nFrames/nfra),100)\n",
    "        sample = torch.FloatTensor(numBatches,2,10,128*160)\n",
    "        for batchnum in range(numBatches):\n",
    "            for framenum in range(10):\n",
    "\n",
    "                flow = np.load(os.path.join(path,str(framenum)+'.npy'))\n",
    "                flow = np.transpose(flow,(2,0,1))\n",
    "                sample[batchnum,:,framenum,:] = torch.from_numpy(flow.reshape(2,128*160)).type(torch.FloatTensor)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folderName = self.listOfFolders[idx]\n",
    "        frames = self.readData(folderName)\n",
    "        sample = { 'frames': frames }\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HyperParameters for the Network\n",
    "NumOfPoles = 40\n",
    "EPOCH = 300\n",
    "BATCH_SIZE = 1\n",
    "LR = 0.0015\n",
    "gpu_id = 1  # 3?\n",
    "\n",
    "## For training UCF\n",
    "# Input -  3 Optical Flow\n",
    "# Output - 1 Optical Flow\n",
    "## For training Kitti\n",
    "# Input -  9 Optical Flow\n",
    "# Output - 1 Optical Flow\n",
    "\n",
    "FRA = 9 # input number of frame\n",
    "PRE = 1 # output number of frame\n",
    "N_FRAME = FRA+PRE\n",
    "N = NumOfPoles*4\n",
    "T = FRA # number of row in dictionary(same as input number of frame)\n",
    "saveEvery = 2\n",
    "N_FRAME_FOLDER = 18\n",
    "\n",
    "#mnist\n",
    "# x_fra = 64\n",
    "# y_fra = 64\n",
    "\n",
    "## Load saved model\n",
    "load_ckpt = False\n",
    "ckpt_file = 'MS_Model_4px_22.pth' # for Kitti Dataset: 'KittiModel.pth'\n",
    "# checkptname = \"UCFModel\"\n",
    "checkptname = \"Kitti_GL_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load input data\n",
    "\n",
    "# set train list name:\n",
    "#trainFolderFile = './datasets/DisentanglingMotion/importing_data/moving_symbols/MovingSymbols2_trainlist.txt'\n",
    "# trainFolderFile = 'trainlist01.txt'\n",
    "\n",
    "# set training data directory:\n",
    "rootDir = '/home/armandcomas/DYAN/Code/datasets/Kitti_Flows/'\n",
    "# rootDir = './datasets/UCF-101-Frames'\n",
    "\n",
    "#trainFoldeList = getListOfFolders(trainFolderFile)[::10]\n",
    "# if Kitti dataset: use listOfFolders instead of trainFoldeList\n",
    "trainFoldeList = [name for name in os.listdir(rootDir) if os.path.isdir(os.path.join(rootDir, name))]\n",
    "\n",
    "trainingData = videoDataset(listOfFolders=trainFoldeList,\n",
    "                            rootDir=rootDir)\n",
    "\n",
    "dataloader = DataLoader(trainingData,\n",
    "                        batch_size=BATCH_SIZE ,\n",
    "                        shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing r, theta\n",
    "P,Pall = gridRing(N)\n",
    "Drr = abs(P)\n",
    "Drr = torch.from_numpy(Drr).float()\n",
    "Dtheta = np.angle(P)\n",
    "Dtheta = torch.from_numpy(Dtheta).float()\n",
    "# What and where is gamma\n",
    "\n",
    "## Create the model\n",
    "model = OFModel(Drr, Dtheta, T, PRE, gpu_id)\n",
    "model.cuda(gpu_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1) # if Kitti: milestones=[100,150], UCF [50,100]\n",
    "loss_mse = nn.MSELoss()\n",
    "start_epoch = 1\n",
    "\n",
    "## If want to continue training from a checkpoint\n",
    "if(load_ckpt):\n",
    "    loadedcheckpoint = torch.load(ckpt_file)\n",
    "    start_epoch = loadedcheckpoint['epoch']\n",
    "    model.load_state_dict(loadedcheckpoint['state_dict'])\n",
    "    optimizer.load_state_dict(loadedcheckpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training from epoch: ', 1)\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception NameError: \"global name 'FileNotFoundError' is not defined\" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f9870edcc50>> ignored\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'Softshrink' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8f9fb3944e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# print(inputData.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# torchvision.utils.save_image(output[:, FRA].view(2, x_sz, y_sz), 'predicted_output.png', )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/armandcomas/DYAN/Spacial_GL/DyanOF.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/armandcomas/DYAN/Spacial_GL/DyanOF.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0msparsecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfista\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparsecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/armandcomas/DYAN/Spacial_GL/DyanOF.py\u001b[0m in \u001b[0;36mfista\u001b[0;34m(D, Y, lambd, maxIter, gpu_id)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftshrink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDtY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mt_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mt_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0my_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0my_old\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'Softshrink' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Training from epoch: \", start_epoch)\n",
    "print('-' * 25)\n",
    "start = time.time()\n",
    "\n",
    "count = 0\n",
    "## Start the Training\n",
    "\n",
    "for epoch in range(start_epoch, EPOCH+1):\n",
    "    loss_value = []\n",
    "    # exp_lr_scheduler.step()\n",
    "    for i_batch, sample in enumerate(dataloader):\n",
    "        dataBatch = sample['frames'].squeeze(0)\n",
    "        numBatches = dataBatch.shape[0]\n",
    "        los_val = []\n",
    "        for batchnum in range(numBatches):\n",
    "            data = dataBatch[batchnum].cuda(1)\n",
    "            expectedOut = data\n",
    "            inputData = data[:,0:9,:]\n",
    "            # print(torch.max(inputData),torch.min(inputData))\n",
    "            # inputData = inputData\n",
    "            # print(inputData.shape)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(inputData)\n",
    "\n",
    "            # torchvision.utils.save_image(output[:, FRA].view(2, x_sz, y_sz), 'predicted_output.png', )\n",
    "            # torchvision.utils.save_image(expectedOut[:, FRA].view(2, x_sz, y_sz), 'expected_output.png', )\n",
    "\n",
    "\n",
    "            loss = loss_mse(output, expectedOut)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            los_val.append(loss.data[0])\n",
    "        loss_value.append(np.mean(np.array(los_val)))\n",
    "    loss_val = np.mean(np.array(loss_value))\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, EPOCH+1):\n",
    "#     loss_value = []\n",
    "#     scheduler.step()\n",
    "#     for i_batch, sample in enumerate(dataloader):\n",
    "#         for n in range(N_FRAME_FOLDER-N_FRAME):\n",
    "            \n",
    "#             data = sample['frames'].squeeze(0).cuda(gpu_id)\n",
    "#             expectedOut = Variable(data)\n",
    "            \n",
    "#             inputData = Variable(data[:,n:(n+FRA),:])\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model.forward(inputData)\n",
    "#             loss = loss_mse(output[:,FRA], expectedOut[:,n+FRA])\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             loss_value.append(loss.data.item())\n",
    "\n",
    "#             # Visualize expected and output images.\n",
    "# #             po = output.data.cpu().numpy()\n",
    "# #             eo = expectedOut.data.cpu().numpy()\n",
    "# #             tmp1 = np.zeros([64, 64, 3], dtype=np.float16)\n",
    "# #             tmp1[:, :, 0] = po[0, FRA, :].reshape(x_fra, y_fra)\n",
    "# #             tmp1[:, :, 1] = po[1, FRA, :].reshape(x_fra, y_fra)\n",
    "\n",
    "# #             scipy.misc.imsave('predicted_outputOF.png', tmp1)\n",
    "\n",
    "# #             tmp2 = np.zeros([64, 64, 3], dtype=np.float16)\n",
    "# #             tmp2[:, :, 0] = eo[0, n+FRA, :].reshape(x_fra, y_fra)\n",
    "# #             tmp2[:, :, 1] = eo[1, n+FRA, :].reshape(x_fra, y_fra)\n",
    "\n",
    "# #             scipy.misc.imsave('expected_outputOF.png', tmp2)\n",
    "\n",
    "#     loss_val = np.mean(np.array(loss_value))\n",
    "\n",
    "    if epoch % saveEvery ==0 :\n",
    "        save_checkpoint({\t'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'optimizer' : optimizer.state_dict(),\n",
    "                            },checkptname+str(epoch)+'.pth')\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        print(model.state_dict()['l1.rr'])\n",
    "        print(model.state_dict()['l1.theta'])\n",
    "        # loss_val = float(loss_val/i_batch)\n",
    "    print('Epoch: ', epoch, '| train loss: %.4f' % loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
